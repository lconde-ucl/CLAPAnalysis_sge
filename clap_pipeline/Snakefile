'''
Aim: A Snakemake workflow to process CLIP and CLAP data
'''

import json
import os 
import sys
import numpy as np
import datetime
from pathlib import Path


##############################################################################
#Initialize settings
##############################################################################


#Copy config file into logs
v = datetime.datetime.now()
run_date = v.strftime('%Y.%m.%d.')

try:
    config_path = config["config_path"]
except:
    config_path = 'config.yaml'

configfile: config_path

try:
    email = config['email']
except:
    email = None
    print("Will not send email on error")


##############################################################################
#Location of scripts
##############################################################################

split_fastq = "scripts/split_fastq.sh"
add_chr = "scripts/ensembl2ucsc.py"
repeat_mask = "scripts/FilterBlacklist.jar"

##############################################################################
#General settings
##############################################################################

try:
    assembly = config['assembly']
    assert assembly in ['mixed', 'mm10', 'hg38'], 'Only "mixed" or "mm10" or "hg38" currently supported'
    print('Using', assembly)
except:
    print('Config "assembly" not specified, defaulting to "mm10"')
    assembly = 'mm10'

try:
    samples = config['samples']
    print('Using samples file:', samples)
except:
    samples = './samples.json'
    print('Defaulting to working directory for samples json file')

try:
    out_dir = config['output_dir']
    print('All data will be written to:', out_dir)
except:
    out_dir = ''
    print('Defaulting to working directory as output directory')

try:
    resolution = config['bin_size']
    print('Making bigwigs of bin size: ', resolution)
except:
    resolution = 1000
    print('Defaulting to bin size of 1000')

try:
    temp_dir = config['temp_dir']
    print("Using temporary directory:", temp_dir)
except:
    temp_dir = '/central/scratch/'
    print('Defaulting to central scratch as temporary directory')

try:
    num_chunks = config['num_chunks']
except:
    num_chunks = 2

try:
    gtf_path = config['gtf_path'][config['assembly']]
except:
    print('GTF file not specified')

try:
    repeat_bed = config['repeat_bed'][config['assembly']]
except:
    print('Repeat bed not specified in config.yaml')
    sys.exit() #no default, exit

##############################################################################
##Trimming Sequences
##############################################################################

try:
    adapters = "-g file:" + config['cutadapt_dpm']
    print('Using cutadapt sequence file', adapters)
except:
    adapters = "-g GGTGGTCTTT -g GCCTCTTGTT \
        -g CCAGGTATTT -g TAAGAGAGTT -g TTCTCCTCTT -g ACCCTCGATT"
    print("No file provided for cutadapt. Using standard cutadapt sequences")

################################################################################
#Aligner Indexes
################################################################################

#RNA aligner
try:
    bowtie2_index = config['bowtie2_index'][config['assembly']]
except:
    print('Bowtie2 index not specified in config.yaml')
    sys.exit() #no default, exit

try:
    star_index = config['star_index'][config['assembly']]
except:
    print('Star index not specified in config.yaml')
    sys.exit() #no default, exit


################################################################################
#make output directories (aren't created automatically on cluster)
################################################################################

Path(out_dir + "workup/logs/cluster").mkdir(parents=True, exist_ok=True)
out_created = os.path.exists(out_dir + "workup/logs/cluster")
print('Output logs path created:', out_created)

################################################################################
#Get sample files
###############################################################################

#Prep samples from fastq directory using fastq2json_updated.py, now load json file
FILES = json.load(open(samples))
ALL_SAMPLES = sorted(FILES.keys())

ALL_FASTQ = []
for SAMPLE, file in FILES.items():
    ALL_FASTQ.extend([os.path.abspath(i) for i in file.get('R1')])
    ALL_FASTQ.extend([os.path.abspath(i) for i in file.get('R2')])

CONFIG = [out_dir + "workup/logs/config_" + run_date + "yaml"]

NUM_CHUNKS = [f"{i:03}" for i in np.arange(0, num_chunks)]

################################################################################
#Trimming
################################################################################

SPLIT_FQ = expand(out_dir + "workup/splitfq/{sample}_{read}.part_{splitid}.fastq.gz", sample=ALL_SAMPLES, read = ["R1", "R2"], splitid=NUM_CHUNKS)

TRIM = expand([out_dir + "workup/trimmed/{sample}_R1.part_{splitid}_val_1.fq.gz", out_dir + "workup/trimmed/{sample}_R2.part_{splitid}_val_2.fq.gz"], sample = ALL_SAMPLES, splitid=NUM_CHUNKS)
TRIM_LOG = expand(out_dir + "workup/trimmed/{sample}_{read}.part_{splitid}.fastq.gz_trimming_report.txt", sample = ALL_SAMPLES, read = ["R1", "R2"], splitid = NUM_CHUNKS)

################################################################################
#RNA workup
################################################################################

BT2_RNA_ALIGN = expand([out_dir + "workup/alignments/{sample}.part_{splitid}.bowtie2.sorted.mapped.bam",
                        out_dir + "workup/alignments/{sample}.part_{splitid}.bowtie2.sorted.unmapped.bam"],
                        sample=ALL_SAMPLES, splitid=NUM_CHUNKS)

BAM_TO_FQ = expand([out_dir + "workup/fastqs/{sample}.part_{splitid}.bowtie2.unmapped_R1.fq.gz",
                    out_dir + "workup/fastqs/{sample}.part_{splitid}.bowtie2.unmapped_R2.fq.gz"],
                    sample=ALL_SAMPLES, splitid=NUM_CHUNKS)

STAR_ALIGN = expand(out_dir + "workup/alignments/{sample}.part_{splitid}.Aligned.out.sorted.bam",sample=ALL_SAMPLES, splitid=NUM_CHUNKS)

UNALIGNED = expand([out_dir + "workup/unmapped/{sample}_R1.part_{splitid}.unaligned.fastq.gz",
                    out_dir + "workup/unmapped/{sample}_R2.part_{splitid}.unaligned.fastq.gz"],
                    sample=ALL_SAMPLES, splitid=NUM_CHUNKS)

ADD_CHR = expand(out_dir + "workup/alignments/{sample}.part_{splitid}.Aligned.out.sorted.chr.bam", sample=ALL_SAMPLES, splitid=NUM_CHUNKS)

MERGE_STAR_BAMS = expand(out_dir + "workup/alignments/{sample}.merged.star.bam", sample = ALL_SAMPLES)

PICARD_DEDUP = expand(out_dir + "workup/alignments/{sample}.merged.dedup.star.bam", sample = ALL_SAMPLES)

PICARD_INDEX = expand(out_dir + "workup/alignments/{sample}.merged.dedup.star.bam.bai", sample = ALL_SAMPLES)

REPEAT_MASK = expand(out_dir + "workup/alignments/{sample}.merged.dedup.star.masked.bam", sample = ALL_SAMPLES)

MERGE_ALL_BAMS = expand(out_dir + "workup/alignments/{sample}.merged.all.bam", sample = ALL_SAMPLES)

MAPPED_BAMS = expand(out_dir + "workup/alignments/{sample}.merged.mapped.bam", sample = ALL_SAMPLES)

################################################################################
################################################################################
#RULE ALL
################################################################################
################################################################################

rule all:
    input: MAPPED_BAMS

#Send and email if an error occurs during execution
onerror:
    shell('mail -s "an error occurred" ' + email + ' < {log}')

wildcard_constraints:
    sample = "[^\.]+"


##################################################################################
#Trimming and repeat alignment
##################################################################################

rule splitfq:
    input:
        r1 = lambda wildcards: FILES[wildcards.sample]['R1'],
        r2 = lambda wildcards: FILES[wildcards.sample]['R2']
    output:
        temp(expand([(out_dir + "workup/splitfq/{{sample}}_R1.part_{splitid}.fastq"), (out_dir + "workup/splitfq/{{sample}}_R2.part_{splitid}.fastq")],  splitid=NUM_CHUNKS))
    params:
        dir = out_dir + "workup/splitfq",
        prefix_r1 = "{sample}_R1.part_0",
        prefix_r2 = "{sample}_R2.part_0"
    log:
        out_dir + "workup/logs/{sample}.splitfq.log"
    conda:
        "envs/sprite.yaml"
    threads: 
        8
    shell:
        '''
        mkdir -p {params.dir}
        bash {split_fastq} {input.r1} {num_chunks} {params.dir} {params.prefix_r1}
        bash {split_fastq} {input.r2} {num_chunks} {params.dir} {params.prefix_r2}
        '''


rule compress_fastq:
    input:
        r1 = out_dir + "workup/splitfq/{sample}_R1.part_{splitid}.fastq",
        r2 = out_dir + "workup/splitfq/{sample}_R2.part_{splitid}.fastq"
    output:
        r1 = out_dir + "workup/splitfq/{sample}_R1.part_{splitid}.fastq.gz",
        r2 = out_dir + "workup/splitfq/{sample}_R2.part_{splitid}.fastq.gz"
    conda:
        "envs/sprite.yaml"
    threads:
        8
    shell:
        '''
        pigz -p {threads} {input.r1}
        pigz -p {threads} {input.r2}
        '''        

#Trim adaptors
#multiple cores requires pigz to be installed on the system
rule adaptor_trimming_pe:
    input:
        [out_dir + "workup/splitfq/{sample}_R1.part_{splitid}.fastq.gz", 
         out_dir + "workup/splitfq/{sample}_R2.part_{splitid}.fastq.gz"]
    output:
         out_dir + "workup/trimmed/{sample}_R1.part_{splitid}_val_1.fq.gz",
         out_dir + "workup/trimmed/{sample}_R1.part_{splitid}.fastq.gz_trimming_report.txt",
         out_dir + "workup/trimmed/{sample}_R2.part_{splitid}_val_2.fq.gz",
         out_dir + "workup/trimmed/{sample}_R2.part_{splitid}.fastq.gz_trimming_report.txt"
    threads:
        10
    log:
        out_dir + "workup/logs/{sample}.{splitid}.trim_galore.log"
    conda:
        "envs/sprite.yaml"
    shell:
        '''
        if [[ {threads} -gt 8 ]]
        then
            cores=2
        else
            cores=1
        fi

        trim_galore \
        --paired \
        --gzip \
        --cores $cores \
        --quality 20 \
        --fastqc \
        -o {out_dir}workup/trimmed/ \
        {input} &> {log}
        '''

rule bowtie2_align:
    input:
        fq1 = out_dir + "workup/trimmed/{sample}_R1.part_{splitid}_val_1.fq.gz",
        fq2 = out_dir + "workup/trimmed/{sample}_R2.part_{splitid}_val_2.fq.gz"
    output:
        bam = temp(out_dir + "workup/alignments/{sample}.part_{splitid}.bowtie2.bam"),
        mapped = out_dir + "workup/alignments/{sample}.part_{splitid}.bowtie2.sorted.mapped.bam",
        unmapped = out_dir + "workup/alignments/{sample}.part_{splitid}.bowtie2.sorted.unmapped.bam"
    log:
        out_dir + "workup/logs/{sample}.{splitid}.bt2.log"
    threads: 
        10
    conda:
        "envs/sprite.yaml"
    shell:
        '''
        (bowtie2 \
        -p 10 \
        -t \
        -x {bowtie2_index} \
        -1 {input.fq1} -2 {input.fq2} | \
        samtools view -bS -> {output.bam}) &> {log}
        samtools view -b -f 4 {output.bam} | samtools sort -n -o {output.unmapped}
        samtools view -b -F 4 {output.bam} | samtools sort > {output.mapped}
        '''

rule bam_to_fq:
    input: out_dir + "workup/alignments/{sample}.part_{splitid}.bowtie2.sorted.unmapped.bam"
    output: 
        r1 = out_dir + "workup/fastqs/{sample}.part_{splitid}.bowtie2.unmapped_R1.fq.gz",
        r2 = out_dir + "workup/fastqs/{sample}.part_{splitid}.bowtie2.unmapped_R2.fq.gz"
    log:
        out_dir + "workup/logs/{sample}.{splitid}.bam2fq.log"
    threads:
        10
    conda:
        "envs/sprite.yaml"
    shell:
        '''
        samtools fastq -1 {output.r1} -2 {output.r2} -0 /dev/null -s /dev/null -n {input} 
        '''

rule star_align:
    input:
        r1 = out_dir + "workup/fastqs/{sample}.part_{splitid}.bowtie2.unmapped_R1.fq.gz",
        r2 = out_dir + "workup/fastqs/{sample}.part_{splitid}.bowtie2.unmapped_R2.fq.gz"
    output:
        sam = temp(out_dir + "workup/alignments/{sample}.part_{splitid}.Aligned.out.sam"),
        sorted = out_dir + "workup/alignments/{sample}.part_{splitid}.Aligned.out.sorted.bam",
        filtered = temp(out_dir + "workup/alignments/{sample}.part_{splitid}.Aligned.out.bam")
    params:
        STAR_OPTIONS = "--readFilesCommand zcat --outFilterIntronMotifs RemoveNoncanonical --outFilterScoreMin 10 --outFilterMultimapNmax 1 --outFilterType BySJout --outSAMunmapped Within --outReadsUnmapped Fastx", prefix = out_dir + "workup/alignments/{sample}.part_{splitid}."
    log:
        out_dir + "workup/logs/{sample}.{splitid}.star.log"
    threads:
        10
    conda:
        "envs/sprite.yaml"
    shell:
        '''
        (STAR \
        --genomeDir {star_index} \
        --readFilesIn {input.r1} {input.r2} \
        --runThreadN {threads} {params.STAR_OPTIONS} \
        --sjdbGTFfile {gtf_path} \
        --outFileNamePrefix {params.prefix}) &> {log}

        samtools view -@ {threads} -bS {output.sam} > {output.filtered}
        samtools sort -@ {threads} -o {output.sorted} {output.filtered}
        samtools index {output.sorted}
        '''


rule compress_unaligned:
    input:
        r1 = out_dir + "workup/alignments/{sample}.part_{splitid}.Unmapped.out.mate1",
        r2 = out_dir + "workup/alignments/{sample}.part_{splitid}.Unmapped.out.mate2"
    output:
        fq1 = out_dir + "workup/unmapped/{sample}_R1.part_{splitid}.unaligned.fastq.gz",
        fq2 = out_dir + "workup/unmapped/{sample}_R2.part_{splitid}.unaligned.fastq.gz"
    conda:
        "envs/sprite.yaml"
    threads:
        8
    shell:
        '''
        pigz -p {threads} {input.r1} {input.r2}
       
        mv {input.r1}.gz {output.fq1}
        mv {input.r2}.gz {output.fq2}
        '''        

rule add_chr:
    input:
        out_dir + "workup/alignments/{sample}.part_{splitid}.Aligned.out.sorted.bam",
    output:
        out_dir + "workup/alignments/{sample}.part_{splitid}.Aligned.out.sorted.chr.bam"
    log:
        out_dir + "workup/logs/{sample}.part_{splitid}.add_chr.log",
    conda:
        "envs/sprite.yaml"
    shell:
        '''
        python {add_chr} -i {input} -o {output} --assembly {assembly} &> {log}
        '''

rule merge_star_bams:
    input:
        expand(out_dir + "workup/alignments/{{sample}}.part_{splitid}.Aligned.out.sorted.chr.bam", splitid=NUM_CHUNKS)
    output:
        out_dir + "workup/alignments/{sample}.merged.star.bam"
    conda:
        "envs/sprite.yaml"
    threads:
        8
    log:
        out_dir + "workup/logs/{sample}.merge_star_bams.log"
    shell:
        '''
        (samtools merge -@ {threads} {output} {input}) &> {log}
        samtools index {output}
        '''

rule picard_dedup:
    input:
        out_dir + "workup/alignments/{sample}.merged.star.bam"
    output:
        bam = out_dir + "workup/alignments/{sample}.merged.dedup.star.bam",
        log = out_dir + "workup/alignments/{sample}.duplicates.out"
    conda:
        "envs/sprite.yaml"
    threads:
        8
    log:
        out_dir + "workup/logs/{sample}.picard.dedup.log"
    shell:
        '''
        picard -Xmx20g  MarkDuplicates I={input} O={output.bam} M={output.log} REMOVE_DUPLICATES=true
        '''

rule picard_index:
    input:
        out_dir + "workup/alignments/{sample}.merged.dedup.star.bam"
    output:
        out_dir + "workup/alignments/{sample}.merged.dedup.star.bam.bai"
    conda:
        "envs/sprite.yaml"
    threads:
        1
    log:
        out_dir + "workup/logs/{sample}.picard.dedup.index.log"
    shell:
        '''
        samtools index {input}
        '''

rule repeat_mask:
    input:
        out_dir + "workup/alignments/{sample}.merged.dedup.star.bam"
    output:
        out_dir + "workup/alignments/{sample}.merged.dedup.star.masked.bam"
    conda:
        "envs/sprite.yaml"
    threads:
        10
    log:
        out_dir + "workup/logs/{sample}.repeatmask.log"
    shell:
        '''
	java -Xmx20g -Dsnappy.disable=true -jar {repeat_mask} {input} {repeat_bed} {output}
        #bedtools intersect -v -abam {input} -b {repeat_bed} > {output}
	'''

rule merge_all_bams:
    input:
        bt2 = expand(out_dir + "workup/alignments/{{sample}}.part_{splitid}.bowtie2.sorted.mapped.bam", splitid=NUM_CHUNKS),
        star = out_dir + "workup/alignments/{sample}.merged.dedup.star.masked.bam"
    output:
        out_dir + "workup/alignments/{sample}.merged.all.bam"
    conda:
        "envs/sprite.yaml"
    threads:
        8
    log:
        out_dir + "workup/logs/{sample}.merge_all_bams.log"
    shell:
        '''
        (samtools merge -@ {threads} {output} {input.bt2} {input.star}) &> {log}
        samtools index {output}
        '''

rule mapped_bams:
    input: 
        out_dir + "workup/alignments/{sample}.merged.all.bam"
    output: 
        out_dir + "workup/alignments/{sample}.merged.mapped.bam"
    conda:
        "envs/sprite.yaml"
    threads:
        8
    log:
        out_dir + "workup/logs/{sample}.mapped_bams.log"
    shell:
        '''
        samtools view -@ {threads} -bF 4 {input} > {output}
        samtools index {output}
        '''
